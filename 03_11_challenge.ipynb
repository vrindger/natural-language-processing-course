{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Text Analysis on Great Expectations Novel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports- **Run First**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Bring in text file with our novel\n",
    "textfile = open('sense_and_sensibility.txt', 'r', encoding = \"utf8\")\n",
    "great_expect = textfile.read()\n",
    "\n",
    "print(great_expect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import ldamodel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lowercase words for word cloud\n",
    "word_cloud_text = great_expect.lower()\n",
    "#Remove numbers and alphanumeric words we don't need for word cloud\n",
    "word_cloud_text = re.sub(\"[^a-zA-Z0-9]\", \" \", word_cloud_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Tokenize the data to split it into words\n",
    "tokens = word_tokenize(word_cloud_text)\n",
    "#Remove stopwords\n",
    "tokens = (word for word in tokens if word not in stopwords.words('english'))\n",
    "#Remove short words less than 3 letters in length\n",
    "tokens = (word for word in tokens if len(word) >= 3)\n",
    "#Add word cloud stopwords\n",
    "stopwords_wc = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data cleaning to split data into sentences\n",
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov|edu|me)\"\n",
    "digits = \"([0-9])\"\n",
    "\n",
    "text = \" \" + great_expect + \"  \"\n",
    "text = text.replace(\"\\n\",\" \")\n",
    "text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "if \"...\" in text: text = text.replace(\"...\",\"<prd><prd><prd>\")\n",
    "if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "text = text.replace(\".\",\".<stop>\")\n",
    "text = text.replace(\"?\",\"?<stop>\")\n",
    "text = text.replace(\"!\",\"!<stop>\")\n",
    "text = text.replace(\"<prd>\",\".\")\n",
    "sentences = text.split(\"<stop>\")\n",
    "sentences = [s.strip() for s in sentences]\n",
    "sentences = pd.DataFrame(sentences)\n",
    "sentences.columns = ['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the first few rows of text that are irrelevant for analysis\n",
    "sentences.drop(sentences.index[:59], inplace=True)\n",
    "sentences = sentences.reset_index(drop=True)\n",
    "sentences.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: Analyze Sense and Sensibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create word frequency distribution\n",
    "fdist = %%%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the 40 most common words in the text\n",
    "%%%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization of top 40 most common words in text\n",
    "plt.figure(figsize=(12,6))\n",
    "fdist.plot(%%%%)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Vader sentiment analyzer\n",
    "analyzer = %%%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfom Vader sentiment analysis\n",
    "sentences['compound'] = [analyzer.polarity_scores(x)['%%%%'] for x in sentences['sentence']]\n",
    "sentences['neg'] = [analyzer.polarity_scores(x)['%%%%'] for x in sentences['sentence']]\n",
    "sentences['neu'] = [analyzer.polarity_scores(x)['%%%%'] for x in sentences['sentence']]\n",
    "sentences['pos'] = [analyzer.polarity_scores(x)['%%%%'] for x in sentences['sentence']]\n",
    "sentences.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get number of positive, neutral, and negative sentences\n",
    "positive_sentence = sentences.loc[sentences['%%%%'] > %%%%]\n",
    "negative_sentence = sentences.loc[sentences['%%%%'] < %%%%] \n",
    "neutral_sentence = sentences.loc[sentences['%%%%'] == %%%%]\n",
    "\n",
    "print(sentences.shape)\n",
    "print(len(positive_sentence))\n",
    "print(len(negative_sentence))\n",
    "print(len(neutral_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize Vader sentiment results\n",
    "plt.figure(figsize=(14,6))\n",
    "plt.hist(sentences['%%%%'], bins=50);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
